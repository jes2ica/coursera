### Data Structures
- Point: organize data so that it can be accessed quickly and usefully.
- Examples: lists, stacks, queues, heaps search trees, hashtables, bloom filters, union-find, etc.
- Why so many? => different data structures support different sets of operations => suitable for different types of tasks
- Rule of Thumb: choose the "minimal" data structrue staht supports all the operations you need.

#### Taking it to the next level
- level 0 - "what's a data structure"
- level 1 - cocktail party-level literacy
- level 2 - "this problem calls out for a heap"
- level 3 - "I only use data structures that I wrote myself"

### Heap
- a container of objects that have keys 
  - employer records, network edges, events etc.
- Supported Operations
  - INSERT: add a new object  to a heap
    - Running time: O(log n)
  - EXTRACT-MIN: remove an object in heap with a minimum key value. [ties broken arbitrarily - equally well, EXTRACT MAX]
    - Running time: O(log n)
  - HEAPIFY: n batched Inserts in o(n) time (o(n) time)
  - DELETE: (o(log n) time)
#### Applications: Sorting
- Canonical use of heap: fast way to do repeated minimum
- Example: SelectionSort 
  - o(n) linear scans, o(n^2) runtime on array of length n
- HeapSort
  - insert all n array elements into a heap
  - extract-min to pluck at elements in sorted order
  - running time = 2n heap operations = o(nlogn) time
  - optimal for a "comparison-based" sorting algorithm!
#### Application: Event Manager
- "Priority Queue" - synonym for a heap
- Example: simulation (e.g. for a video game)
  - objects = event records (action/update to occur at given time in the future)
  - key = time event scheduled to occur
  - etract-min => yields the next scheduled event
#### Application: Median Maintanence
- I give you: a sequence x1,...xn of numbers, one-by-one.
- You tell me: at each time step i, the median of {x1, ...xi};
- constraint: use o(log i) time at each step i
- solution: maintain heaps H(low): supports EXTRACT_MAX; H(high): supports EXTRACT_MIN
- key idea: maintain invariant thath ~ i/2 smallest (largest) elements in H(low)/H(high)
- you check:
  - can maintain invariant with o(log i) work
  - given invariant, can compute median in o(log i) work
#### Application: Speeding Up Dijkstra
- Dijkstra's Shortest-Path Algorithm
  - naive implementation => runtime = o(nm) 
    - n: # loop iteration
    - m: work per iteration [linear scan through edges for minimum computations]
  - with heaps => run time = o(m log n)
### Implementation Details
- Conceptually: think of a heap as a tree.
  - rooted, binary, as complete as possible
- Heap property: at every node x, key[x] <= all keys of x's children
- Consequence: object at root must have minimum key value
- Insertion
  - step 1: stick k at end of last level.
  - step 2: bubble-up k until heap property is restored (i.e. key of k's parent is <=k)
  - check: 
    - bubbling up process must stop, with heap property restored
    - runtime = o(log n)
- Extract-min
  - Delete root
  - Move last leaf to be new root
  - Iteratively bubble-down until heap property has been restored
  - check:
    - only bubble-down once per level, halt with a heap
    - runtime = o(log n)
### Sorted Arrays
- Supported Operations
  - search: o(log n)
  - select (given order statistic i): o(1)
  - min/max: o(1)
  - predecessor/successor (given pointer to a key): o(1)
  - rank: i.e. # of keys less than or equal to a given value: o(log n)
  - output in sorted order: o(n)
### Balanced Search Trees
- Supported Operations
  - search: o(log n)
  - select (given order statistic i):o(log n)
  - min/max: o(log n)
  - predecessor/successor (given pointer to a key): o(log n)
  - rank: i.e. # of keys less than or equal to a given value: o(log n)
  - output in sorted order: o(n)
  - insert/delete o(log n)
- Structure
  - exact one node per key
  - most basic version, each node has:
    - left child pointer
    - right child pointer
    - parent pointer
- The height of a BST
  - Note: many possible trees for a set of keys.
  - height(depth=longest root-leaf path) could be anywhere from ~log2n(best case, perfectly balanced) to ~n(worst case)
- Searching and Inserting
  - To search for key k in tree t
    - start at the root
    - traverse left(if k < key at current node)/right(if k > key at current node) child pointer as needed
    - return node with key k or null, as appropriate
  - To insert a new key k into a tree t
    - search for k (unsuccessfully)
    - rewire final null pointer to point to new node with key k
    - the height governs worst-case running time
- Min, Max, Pred and Succ
  - Min/Max
    - start at the root
    - follow left/right child pointers until you can't anymore (return last key found)
  - Predecessor
    - easy case: if k's left substree is not empty, return max key in left subtree
- In-Order Traversal
  - Print out keys in increasing order
    - let r = root of search tree, with subtrees T(l) and T(r)
    - recurse on T(l)
      - print out keys of T(l) in increasing order
    - print out r's key
    - recurse on T(r) 
      - print out keys of T(r) in increasing order
    - otherwise: follow parent pointers until you get to a key less than k
- Deletion
  - To detele a key k from a search tree
    - search for k
      - easy case (k has no children)
        - just dilete k's node from tree, done
      - medium case (k's node has one child)  
        - just "splice out" k's node (unique child assumes position previously held by k's node)
      - difficult case (k's node has 2 children)
        - compute k's predecessor l (i.e. traverse k's non-NULL left child ptr, then right child ptrs until no longer possible)
        - swap k and l!
          - Note: in its new position, k has no right child! => easy to delete or splice out k's new node
          - Exercise: at end, we have a valid search tree!
    - Running time: o(height)
- Select and Rank
  - Idea: store a little bit of extra info at each tree node about the tree itself (i.e. not about the data)
  - Example augmentation: size(x) = # of treenodes in subtree rooted at x.
  - Note: if x has children y and z, then size(x) = size(y) + size(z) + 1
  - Also: easy to keep sizes up-to-date during an Insetion or Deletion
  - How to SELECT ith order statistic from augmented search tree (with subtree sizes)
    - start at root x, with children y and z.
    - let a = size(y) l(a) = 0 if x has no left child
    - if a = i-1, return x's key
    - if a >= i, recursively compute ith order statistic of search tree rooted at y
    - if a < i, recursively compute (i-a-1) # order statistic of saerch tree rooted at z
  - Running time = O(height)
#### Red-black Trees
(see also AVL trees, splaytrees, B trees)
- Invariants
  - each node red or black
  - root is black
  - no 2 reds in a row [red node => only black children]
  - every root-null path has same number of black nodes (like in an unsuccessful search)
- Height Guarantee
  - Claim: every red-black tree with n nodes has height <= 2log2(n + 1)
  - Proof: 
    - observation: if every root-NULL path has >= k nodes, then tree includes (at the top) a perfectly blanced search tree of depth k 
    - => size n of the tree must be at least 2^k - 1, k = minimum # of nodes on root-NULL path
    - k <= log2(n + 1)
    - Thus: in a red-black tree with n nodes, there is a root-NULL path with at most log2(n+1) black nodes.
    - By 4th invariant: every root-NULL path has <= log2(n+1) black nodes
    - By 3th invariant: every root-NULL path has <= 2log2(n+1) total nodes.
- Rotations
  - Key primitive: rotations. (common to all balanced search tree implementations: red-black, AVL, B+, etc)
  - Idea: locally rebalance subtrees at a node in o(1) time.
  - Properties: search tree property maintain, can implement in o(1) time.
- Insertion
  - Idea for Insert / Delete: proceed as in a normal binary search tree, then recolor and/or perform rotations until invariants are restored.
  - Insert(x): 
    - insert x as usual (make x a leaf)
    - try coloring x red
    - if x's parent y is black, done.
    - else y is red => y has a black parent w 
      - case 1: the other child z of x's grandparent w is also red.
        - recolor y, z black and w red
        - by point: does not break invariant 4
        - either restores invariant 3 or prepagates the double red upward.
        - can only happen o(log n) times
      - case 2: the other child z of x's grandparent w is NULL or is a black.
        - Exercise / case analysis: can eliminate double-red [=> all invariants satisfied] in o(1) time via 2-3 rotations + recolorings.
